{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed articles and tags\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "fa = open('new_preprocessed_articles.txt', 'r', encoding='utf8')\n",
    "ft = open('new_preprocessed_articles_tags.txt', 'r', encoding='utf8')\n",
    "\n",
    "tags_set = set()\n",
    "tags_for_articles = []\n",
    "\n",
    "processed_articles = {'content': [re.sub('\\\\n', '', str(article)) for article in fa], 'tags': []}\n",
    "for tag in ft:\n",
    "    corresponding_tags = re.split(', ', str(tag).replace('[', '').replace(']', '').replace('\\n', ''))\n",
    "    processed_articles['tags'].append(corresponding_tags)\n",
    "    tags_set.update(corresponding_tags)\n",
    "    tags_for_articles.extend(corresponding_tags)\n",
    "\n",
    "fa.close()\n",
    "ft.close()\n",
    "\n",
    "df_articles = pd.DataFrame(data=processed_articles)\n",
    "\n",
    "# calculate tag score by its frequency distribution. score = occurrance / total number of article tags\n",
    "tag_scores = {}\n",
    "for tag in tags_set:\n",
    "    tag_scores[tag] = tags_for_articles.count(tag)/len(tags_for_articles)\n",
    "\n",
    "del tags_for_articles\n",
    "del tags_set\n",
    "\n",
    "# calculate article score according to the sum of its tag score and sort the articles by score\n",
    "score = []\n",
    "for i, row in df_articles.iterrows():\n",
    "    score.append(sum([tag_scores[tag] for tag in row['tags']]))\n",
    "df_articles['score'] = score\n",
    "df_articles.sort_values(by='score', ascending=False)\n",
    "\n",
    "del score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the article.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(df_articles.tags[0:100000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_articles.content[0:100000], binarized_labels, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are “continuous bag of words” (cbow) and “skip-gram” (sg); in the Doc2Vec architecture, the corresponding algorithms are “distributed bag of words” (dbow) and “distributed memory” (dm).\n",
    "\n",
    "## DBOW\n",
    "\n",
    "DBOW is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "Training a Doc2Vec model is rather straight forward in Gensim, we initialize the model and train for 30 epochs:\n",
    "\n",
    "dm=0 means ‘distributed bag of words’ (DBOW), set min_count=2 means ignoring all words with total frequency lower than this, size=300 is dimensionality of the generated feature vectors, alpha=0.065 is the initial alpha rate, learning rate will linearly drop to min_alpha as training progresses. And then we build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 100000/100000 [00:00<00:00, 2628734.74it/s]\n"
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=2, epoches=30, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 100000/100000 [00:00<00:00, 3333548.45it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571352.91it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3333283.53it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448299.00it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571383.32it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571383.32it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3703775.92it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3446853.76it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571596.20it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3704004.87it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571596.20it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3703710.51it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3444419.44it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3575798.19it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3451732.74it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571413.73it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3573482.83it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3567252.38it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3704364.72it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448299.00it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571322.50it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571474.55it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571504.96it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571504.96it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3447788.77it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571444.14it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571413.73it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571535.38it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3568344.93it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571170.47it/s]\nWall time: 9min 54s\n"
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dbow_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Memory\n",
    "\n",
    "Distributed Memory (DM) acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "\n",
    "We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 100000/100000 [00:00<00:00, 3448412.40it/s]\n"
    }
   ],
   "source": [
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=2, epoches=30, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dm.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 100000/100000 [00:00<00:00, 3226362.87it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3333230.55it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571292.09it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571474.55it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3567555.80it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571352.91it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3444391.16it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571535.38it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571352.91it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3567222.04it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448440.75it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3333415.99it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3447958.83it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3333310.02it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571140.06it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448270.65it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3225668.12it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571444.14it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3567555.80it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3566767.01it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3447647.07it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448327.35it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3703874.04it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571596.20it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571444.14it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448213.95it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571596.20it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3229443.24it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3448327.35it/s]\n100%|██████████| 100000/100000 [00:00<00:00, 3571474.55it/s]\nWall time: 12min 26s\n"
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dm.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dm_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get document vectors from doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dbow = Doc2Vec.load('d2v_dbow_model.doc2vec')\n",
    "# model_dm = Doc2Vec.load('d2v_dm_model.doc2vec')\n",
    "\n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n",
    "\n",
    "train_vectors_dm = get_vectors(model_dm, len(X_train), 300, 'Train')\n",
    "test_vectors_dm = get_vectors(model_dm, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pairing\n",
    "\n",
    "combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1, model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix], model2.docvecs[prefix])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_train), 600, 'Train')\n",
    "test_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_test), 600, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier. Here we try to use SGDClassifier for large-scale corpus datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 12h 55min 38s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('clf',\n                 OneVsRestClassifier(estimator=LogisticRegression(C=1.0,\n                                                                  class_weight=None,\n                                                                  dual=False,\n                                                                  fit_intercept=True,\n                                                                  intercept_scaling=1,\n                                                                  l1_ratio=None,\n                                                                  max_iter=100,\n                                                                  multi_class='auto',\n                                                                  n_jobs=None,\n                                                                  penalty='l2',\n                                                                  random_state=None,\n                                                                  solver='sag',\n                                                                  tol=0.0001,\n                                                                  verbose=0,\n                                                                  warm_start=False),\n                                     n_jobs=-1))],\n         verbose=False)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = Pipeline([\n",
    "    # (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    # (\"lsa\", TruncatedSVD()),\n",
    "    # ('clf', OneVsRestClassifier(SGDClassifier(), n_jobs=-1))\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1))\n",
    "])\n",
    "\n",
    "classifier.fit(train_vecs_dbow_dm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(test_vecs_dbow_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score: 0.0360666667\nF1 score: 0.3730349006\n"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"Accuracy score: %.10f\" % accuracy_score(y_test, predicted))\n",
    "# 0.0000333333 for SGD classifier (100000 articles)\n",
    "# 0.0360666667 for LogisticRegression (100000 articles)\n",
    "print(\"F1 score: %.10f\" % f1_score(y_test, predicted, average='micro'))\n",
    "# 0.0000578010 for SGD classifier (100000 articles)\n",
    "# 0.3730349006 for LogisticRegression (100000 articles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}