{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed articles and tags\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "fa = open('new_preprocessed_articles.txt', 'r', encoding='utf8')\n",
    "ft = open('new_preprocessed_articles_tags.txt', 'r', encoding='utf8')\n",
    "\n",
    "tags_set = set()\n",
    "tags_for_articles = []\n",
    "\n",
    "processed_articles = {'content': [re.sub('\\\\n', '', str(article)) for article in fa], 'tags': []}\n",
    "for tag in ft:\n",
    "    corresponding_tags = re.split(', ', str(tag).replace('[', '').replace(']', '').replace('\\n', ''))\n",
    "    processed_articles['tags'].append(corresponding_tags)\n",
    "    tags_set.update(corresponding_tags)\n",
    "    tags_for_articles.extend(corresponding_tags)\n",
    "\n",
    "fa.close()\n",
    "ft.close()\n",
    "\n",
    "df_articles = pd.DataFrame(data=processed_articles)\n",
    "\n",
    "# calculate tag score by its frequency distribution. score = occurrance / total number of article tags\n",
    "tag_scores = {}\n",
    "for tag in tags_set:\n",
    "    tag_scores[tag] = tags_for_articles.count(tag)/len(tags_for_articles)\n",
    "\n",
    "del tags_for_articles\n",
    "del tags_set\n",
    "\n",
    "# calculate article score according to the sum of its tag score and sort the articles by score\n",
    "score = []\n",
    "for i, row in df_articles.iterrows():\n",
    "    score.append(sum([tag_scores[tag] for tag in row['tags']]))\n",
    "df_articles['score'] = score\n",
    "df_articles.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the article.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(df_articles.tags)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_articles.content, binarized_labels, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are “continuous bag of words” (cbow) and “skip-gram” (sg); in the Doc2Vec architecture, the corresponding algorithms are “distributed bag of words” (dbow) and “distributed memory” (dm).\n",
    "\n",
    "## DBOW\n",
    "\n",
    "DBOW is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "Training a Doc2Vec model is rather straight forward in Gensim, we initialize the model and train for 30 epochs:\n",
    "\n",
    "dm=0 means ‘distributed bag of words’ (DBOW), set min_count=2 means ignoring all words with total frequency lower than this, size=300 is dimensionality of the generated feature vectors, alpha=0.065 is the initial alpha rate, learning rate will linearly drop to min_alpha as training progresses. And then we build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=2, epoches=30, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dbow_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Memory\n",
    "\n",
    "Distributed Memory (DM) acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "\n",
    "We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=2, epoches=30, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dm.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dm.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dm_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get document vectors from doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dbow = Doc2Vec.load('d2v_dbow_model.doc2vec')\n",
    "# model_dm = Doc2Vec.load('d2v_dm_model.doc2vec')\n",
    "\n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n",
    "\n",
    "train_vectors_dm = get_vectors(model_dm, len(X_train), 300, 'Train')\n",
    "test_vectors_dm = get_vectors(model_dm, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pairing\n",
    "\n",
    "combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1, model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix], model2.docvecs[prefix])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_train), 600, 'Train')\n",
    "test_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_test), 600, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier. Here we try to use SGDClassifier for large-scale corpus datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "classifier = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"lsa\", TruncatedSVD(n_components=1000)),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(), n_jobs=-1))\n",
    "])\n",
    "\n",
    "classifier.fit(train_vecs_dbow_dm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(test_vecs_dbow_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "print \"Accuracy score: %.5f\" % accuracy_score(y_test, predicted)\n",
    "print \"F1 score: %.5f\" % f1_score(y_test, predicted, average='micro')"
   ]
  }
 ]
}