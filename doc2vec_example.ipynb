{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed articles and tags\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "fa = open('new_preprocessed_articles.txt', 'r', encoding='utf8')\n",
    "ft = open('new_preprocessed_articles_tags.txt', 'r', encoding='utf8')\n",
    "\n",
    "tags_set = set()\n",
    "tags_for_articles = []\n",
    "\n",
    "processed_articles = {'content': [re.sub('\\\\n', '', str(article)) for article in fa], 'tags': []}\n",
    "for tag in ft:\n",
    "    corresponding_tags = re.split(', ', str(tag).replace('[', '').replace(']', '').replace('\\n', ''))\n",
    "    processed_articles['tags'].append(corresponding_tags)\n",
    "    tags_set.update(corresponding_tags)\n",
    "    tags_for_articles.extend(corresponding_tags)\n",
    "\n",
    "fa.close()\n",
    "ft.close()\n",
    "\n",
    "df_articles = pd.DataFrame(data=processed_articles)\n",
    "\n",
    "# calculate tag score by its frequency distribution. score = occurrance / total number of article tags\n",
    "tag_scores = {}\n",
    "for tag in tags_set:\n",
    "    tag_scores[tag] = tags_for_articles.count(tag)/len(tags_for_articles)\n",
    "\n",
    "del tags_for_articles\n",
    "del tags_set\n",
    "\n",
    "# calculate article score according to the sum of its tag score and sort the articles by score\n",
    "score = []\n",
    "for i, row in df_articles.iterrows():\n",
    "    score.append(sum([tag_scores[tag] for tag in row['tags']]))\n",
    "df_articles['score'] = score\n",
    "df_articles.sort_values(by='score', ascending=False)\n",
    "\n",
    "del score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the article.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(df_articles.tags[0:100000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_articles.content[0:100000], binarized_labels, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are “continuous bag of words” (cbow) and “skip-gram” (sg); in the Doc2Vec architecture, the corresponding algorithms are “distributed bag of words” (dbow) and “distributed memory” (dm).\n",
    "\n",
    "## DBOW\n",
    "\n",
    "DBOW is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "Training a Doc2Vec model is rather straight forward in Gensim, we initialize the model and train for 30 epochs:\n",
    "\n",
    "dm=0 means ‘distributed bag of words’ (DBOW), set min_count=2 means ignoring all words with total frequency lower than this, size=300 is dimensionality of the generated feature vectors, alpha=0.065 is the initial alpha rate, learning rate will linearly drop to min_alpha as training progresses. And then we build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=2, epoches=30, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dbow_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Memory\n",
    "\n",
    "Distributed Memory (DM) acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "\n",
    "We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=2, epoches=30, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dm.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dm.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dm_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get document vectors from doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('d2v_dbow_model.doc2vec')\n",
    "model_dm = Doc2Vec.load('d2v_dm_model.doc2vec')\n",
    "\n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n",
    "\n",
    "train_vectors_dm = get_vectors(model_dm, len(X_train), 300, 'Train')\n",
    "test_vectors_dm = get_vectors(model_dm, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pairing\n",
    "\n",
    "combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1, model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix], model2.docvecs[prefix])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_train), 600, 'Train')\n",
    "test_vecs_dbow_dm = get_concat_vectors(model_dbow, model_dm, len(X_test), 600, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier. Here we try to use SGDClassifier for large-scale corpus datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 26min 22s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('standardscaler',\n                 StandardScaler(copy=True, with_mean=False, with_std=True)),\n                ('clf',\n                 OneVsRestClassifier(estimator=SGDClassifier(alpha=0.0001,\n                                                             average=False,\n                                                             class_weight=None,\n                                                             early_stopping=False,\n                                                             epsilon=0.1,\n                                                             eta0=0.0,\n                                                             fit_intercept=True,\n                                                             l1_ratio=0.15,\n                                                             learning_rate='optimal',\n                                                             loss='hinge',\n                                                             max_iter=1000,\n                                                             n_iter_no_change=5,\n                                                             n_jobs=None,\n                                                             penalty='l2',\n                                                             power_t=0.5,\n                                                             random_state=None,\n                                                             shuffle=True,\n                                                             tol=0.001,\n                                                             validation_fraction=0.1,\n                                                             verbose=0,\n                                                             warm_start=False),\n                                     n_jobs=-1))],\n         verbose=False)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = Pipeline([\n",
    "    (\"standardscaler\", StandardScaler(with_mean=False)),\n",
    "    # (\"lsa\", TruncatedSVD()),\n",
    "    # (\"lsa\", TruncatedSVD(n_components=100, n_iter=7, random_state=42)),    \n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(), n_jobs=-1))\n",
    "    # ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1))\n",
    "    # ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n",
    "])\n",
    "\n",
    "classifier.fit(train_vecs_dbow_dm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(test_vecs_dbow_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score: 0.0223333333\nF1 score: 0.3394965056\n"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"Accuracy score: %.10f\" % accuracy_score(y_test, predicted))\n",
    "# 0.0223333333 for SGD classifier (100000 articles) (w/o truncated svd)\n",
    "# 0.0360666667 for LogisticRegression (100000 articles) (w/o truncated svd)\n",
    "print(\"F1 score: %.10f\" % f1_score(y_test, predicted, average='micro'))\n",
    "# 0.3394965056 for SGD classifier (100000 articles) (w/o truncated svd)\n",
    "# 0.3730349006 for LogisticRegression (100000 articles)  (w/o truncated svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search SVD params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "X = StandardScaler().fit_transform(train_vecs_dbow_dm)\n",
    "X_sparse = csr_matrix(X)\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=X_sparse.shape[1]-1)\n",
    "X_tsvd = tsvd.fit(X)\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  }
 ]
}